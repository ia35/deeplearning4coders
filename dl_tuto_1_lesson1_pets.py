# -*- coding: utf-8 -*-
"""dl_tuto_1_lesson1-pets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17cgWkazh0oNHmInjNqFiW-eKt-ghACTX

# Tuto 1 - What's your pet

Cette leçon reprend le cours [fastai](https://course.fast.ai/videos/?lesson=1) de @jeremyphoward : What's your pet

Ce cours a déjà été commenté par nous [ici](http://intelligence-artificielle.agency/notes-sur-la-lecon-1-de-fastai-deep-learning/).  Il s'agit d'aller plus loin dans l'exégèse ! sans reprendre ce qui a déjà été [fait](http://intelligence-artificielle.agency/notes-sur-la-lecon-1-de-fastai-deep-learning/).

Cette leçon vous apprendra à reconnaitre automatiquement, à partir d'une photo, la race de votre chat ou chien

Comme vous le savez (voir leçons précédentes, les notebooks commencent toujours avec les lignes suivantes (dans les cours de fastai) :
"""

# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline

"""fastai supporte 4 domaines d'application du Deep Learning :  

1.   vision
2.   text
3.   tabular
4.   collab  (collaborative filtering)

Lorsqu'on importe fastai on importe en même temps les modules PyTorch
"""

from fastai.vision import *
from fastai.metrics import error_rate

"""Cette remarque ne s'applique pas dans le cas de Google Colab.

*If you're using a computer with an unusually small GPU, you may get an out of memory error when running this notebook. If this happens, click Kernel->Restart, uncomment the 2nd line below to use a smaller *batch size* (you'll learn all about what this means during the course), and try again. *
"""

bs = 64
# bs = 16   # uncomment this line if you run out of memory even after clicking Kernel->Restart

"""## Données

Le jeu de données est  [Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) atrribué à  [O. M. Parkhi et al., 2012](http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf)

Ce jeu comprend des images de chats (12 races) et de chiens (25 races). 
L'exercice consisite à catégoriser correctement une image.

Il y a donc en tout 37 catégories et chaque classe a environ 200 images. Les images sont très variées.

En 2012, le meilleurs score était de 59.21%, avec un logiciel dédié aux chats/chiens.

Ces données sont aussi sur [Kaggle](https://www.kaggle.com/c/oxford-iiit-pet-dataset/overview). 

La métrique recommandée est Multi Class Loss

Lorsqu'on importe fastai, on peut uitliser URLs : Global constants for dataset and model URLs
"""

??URLs

"""On utilise la fonction untar_data à qui on passe en paramètre une URL. Cette fonction télécharge les données et les décompresse"""

help(untar_data)

"""Notez que `untar_data` retourne un objet de type [pathlib](https://docs.python.org/3/library/pathlib.html)"""

path = untar_data(URLs.PETS); path

type(path)

path.ls()

"""Python 3 permet de faire de la concaténation de path de la manière suivante :"""

path_anno = path/'annotations'
path_img = path/'images'

path_anno.ls()

import os
dirs = os.listdir( path_img )

"""Pour voir la liste des 10 premiers fichiers

(on peut aussi utiliser la fonction fastai get_image_files)
"""

dirs[0:10]

"""Les étiquettes (labels) des images sont déduites des noms des fichiers.

Par exemple, boxer_46.jpg est un boxer.

Les noms des fichiers sont ici sous la forme d'expressions régulières : race_nombre.ext

fastai a un outil pour ça : `ImageDataBunch.from_name_re` retourne les labels extraits des noms de fichiers à l'aide d'une [expression régulière](https://docs.python.org/3.6/library/re.htm)
"""

??get_image_files

fnames = get_image_files(path_img)
fnames[:5]

np.random.seed(2)

pat = r'/([^/]+)_\d+.jpg$'

"""Une petite explication de l'expression régulière s'impose, elle veut dire une expression qui commence par un / suivi de tout sauf un slash, suivi de _ puis d'un nombre et de .jpg qui termine l'expression

en plaçant un r' avant le délimiteur qui ouvre notre chaîne, tous les caractères anti-slash qu'elle contient sont échappés

on commence par un /

() crée un groupe, tout sauf un slash [^/] ce qui revient à rechercher le dernier slash /

et pour la fin : _\d+.jpg$

le caractère _ suivi de  \d : chiffre, + : un ou plus donc  \d+ : un ou plusieurs chiffres
 .jpg : suivi de .jpg
 $ : qui termine la ligne

Une autre façon d'écrire l'expression, peut-être plus simple : /[a-z_]*_\d+.jpg$ à condition que seuls ces caractères soient autorisés

Pour ceux qui veulent s'amuser avec les expressions régulières, c'est [ici](https://pythex.org).
"""

data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs
                                  ).normalize(imagenet_stats)

??get_transforms

bs

"""ImageDataBunch.from_name_re : Create from list of `fnames` in `path` with re expression `pat`

get_transform : Utility func to easily create a list of flip, rotate, `zoom`, warp, lighting transforms (on fait du cropping, resizing, ...)

Les images sont retaillées pour être carrées en 224

bs est le batch size

Les images sont aussi normalisées pour avoir la moyenne et l'écart-type de imagenet
"""

??ImageDataBunch.from_name_re

??data.show_batch

data.show_batch(rows=3, figsize=(7,6))

print(data.classes)
len(data.classes),data.c

"""On retrouve bien nos 37 classes  : 12 races de chat et 25 de chiens

Pour calculer l'effectuer de chaque classe dans les données d'apprentissage : 

(méthode proposée [ici](https://forums.fast.ai/t/get-value-counts-from-a-imagedatabunch/38784))
"""

import pandas as pd
vc = pd.value_counts(data.train_ds.y.items, sort=False)
vc.index = data.classes; vc

"""et pour les données de validation"""

vd = pd.value_counts(data.valid_ds.y.items, sort=False)
vd.index = data.classes; vd

"""## Apprentissage: resnet34

On utilise un [réseau de neurones à convolution](http://cs231n.github.io/convolutional-networks/) basé sur une architecture Resnet

L'apprentissage se fait avec 4 epochs (4 cycles sur la totalité des données)
"""

??error_rate

"""error_rate = 1 - `accuracy`"""

learn = cnn_learner(data, models.resnet34, metrics=error_rate)

"""Les poids du modèle ResNet34 sont chargés

[PyTorch](https://pytorch.org/docs/stable/torchvision/models.html) propose plusieurs architectures ResNet (18, 34, 50, 101, 152) en plus d'autres architectures (AlexNet, VGG, SqueezeNet, DenseNet, Inception, GoogLeNet)
"""

learn.model



learn.fit_one_cycle(4)

learn.save('stage-1')

"""## Résultats

Let's see what results we have got. 

We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable (none of the mistakes seems obviously naive). This is an indicator that our classifier is working correctly. 

Furthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour.
"""

interp = ClassificationInterpretation.from_learner(learn)

losses,idxs = interp.top_losses()

len(data.valid_ds)==len(losses)==len(idxs)

interp.plot_top_losses(9, figsize=(15,11))







doc(interp.plot_top_losses)

interp.plot_confusion_matrix(figsize=(12,12), dpi=60)

interp.most_confused(min_val=2)

"""## Unfreezing, fine-tuning, and learning rates

Since our model is working as we expect it to, we will *unfreeze* our model and train some more.
"""

learn.unfreeze()

learn.fit_one_cycle(1)

learn.load('stage-1');

learn.lr_find()

learn.recorder.plot()

learn.unfreeze()
learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4))

"""That's a pretty accurate model!

## Training: resnet50

Now we will train in the same way as before but with one caveat: instead of using resnet34 as our backbone we will use resnet50 (resnet34 is a 34 layer residual network while resnet50 has 50 layers. It will be explained later in the course and you can learn the details in the [resnet paper](https://arxiv.org/pdf/1512.03385.pdf)).

Basically, resnet50 usually performs better because it is a deeper network with more parameters. Let's see if we can achieve a higher performance here. To help it along, let's us use larger images too, since that way the network can see more detail. We reduce the batch size a bit since otherwise this larger network will require more GPU memory.
"""

data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(),
                                   size=299, bs=bs//2).normalize(imagenet_stats)

learn = cnn_learner(data, models.resnet50, metrics=error_rate)

learn.lr_find()
learn.recorder.plot()

learn.fit_one_cycle(8)

learn.save('stage-1-50')

"""It's astonishing that it's possible to recognize pet breeds so accurately! Let's see if full fine-tuning helps:"""

learn.unfreeze()
learn.fit_one_cycle(3, max_lr=slice(1e-6,1e-4))

"""If it doesn't, you can always go back to your previous model."""

learn.load('stage-1-50');

interp = ClassificationInterpretation.from_learner(learn)

interp.most_confused(min_val=2)

"""## Other data formats"""

path = untar_data(URLs.MNIST_SAMPLE); path

tfms = get_transforms(do_flip=False)
data = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=26)

data.show_batch(rows=3, figsize=(5,5))

learn = cnn_learner(data, models.resnet18, metrics=accuracy)
learn.fit(2)

df = pd.read_csv(path/'labels.csv')
df.head()

data = ImageDataBunch.from_csv(path, ds_tfms=tfms, size=28)

data.show_batch(rows=3, figsize=(5,5))
data.classes

data = ImageDataBunch.from_df(path, df, ds_tfms=tfms, size=24)
data.classes

fn_paths = [path/name for name in df['name']]; fn_paths[:2]

pat = r"/(\d)/\d+\.png$"
data = ImageDataBunch.from_name_re(path, fn_paths, pat=pat, ds_tfms=tfms, size=24)
data.classes

data = ImageDataBunch.from_name_func(path, fn_paths, ds_tfms=tfms, size=24,
        label_func = lambda x: '3' if '/3/' in str(x) else '7')
data.classes

labels = [('3' if '/3/' in str(x) else '7') for x in fn_paths]
labels[:5]

data = ImageDataBunch.from_lists(path, fn_paths, labels=labels, ds_tfms=tfms, size=24)
data.classes

